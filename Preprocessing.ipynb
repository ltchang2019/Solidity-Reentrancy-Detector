{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygments.lexers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('./reentrancyContracts')\n",
    "sys.path.append('./reentrancyContractLabels')\n",
    "import reentrancyContracts\n",
    "import reentrancyContractLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in full contract and pull out names of global variables (with reasonable accuracy)\n",
    "def getGlobalVariables(contract):\n",
    "    lexer = pygments.lexers.get_lexer_by_name('Solidity')\n",
    "    tokens = list(pygments.lex(contract, lexer))\n",
    "    globalVarList = []\n",
    "    openBracketsCount = 0\n",
    "    openParenthesisCount = 0\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if str(tokens[i][0]) == 'Token.Text.Whitespace' or str(tokens[i][0]) == 'Token.Comment.Single': None\n",
    "        elif(tokens[i][1] == '{'): openBracketsCount += 1\n",
    "        elif(tokens[i][1] == '}'): openBracketsCount -= 1\n",
    "        elif(tokens[i][1] == '('): openParenthesisCount += 1\n",
    "        elif(tokens[i][1] == ')'): openParenthesisCount -= 1\n",
    "        elif(str(tokens[i][0]) == 'Token.Keyword.Type' and isType(tokens[i][1]) and openBracketsCount == 1 and openParenthesisCount == 0):\n",
    "            index = 0\n",
    "            potentialGlobal = \"\"\n",
    "            include = True\n",
    "            while True:\n",
    "                if(tokens[i+index][1] == \"constant\"): \n",
    "                    include = False\n",
    "                if(tokens[i+index][1] == ';'):\n",
    "                    if include:\n",
    "                        globalVarList.append(potentialGlobal)\n",
    "                    break\n",
    "                if(tokens[i+index][0] == pygments.token.Name.Variable or tokens[i+index][0] == pygments.token.Text):\n",
    "                    potentialGlobal = tokens[i+index][1]\n",
    "                    \n",
    "                index += 1\n",
    "            i += (index-1)\n",
    "        i += 1\n",
    "        \n",
    "    return globalVarList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Modifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts names of modifiers (function guards) for tokenization process\n",
    "def getModifiers(contract):\n",
    "    lexer = pygments.lexers.get_lexer_by_name('Solidity')\n",
    "    tokens = list(pygments.lex(contract, lexer))\n",
    "    filteredTokens = []\n",
    "    for token in tokens:\n",
    "        if str(token[0]) != 'Token.Text.Whitespace':\n",
    "            filteredTokens.append(token)\n",
    "            \n",
    "    modifierNameList = []\n",
    "    openBracketsCount = 0\n",
    "    openParenthesisCount = 0\n",
    "    i = 0\n",
    "    while i < len(filteredTokens):\n",
    "        if(str(filteredTokens[i][0]) == 'Token.Keyword.Type' and str(filteredTokens[i][1]) == 'modifier'):\n",
    "            modifierNameList.append(filteredTokens[i+1][1])\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return modifierNameList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['onlyOwner', 'isOpenToPublic', 'onlyRealPeople', 'onlyPlayers']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getModifiers(reentrancyContracts.contracts[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Function Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given contract source code, extracts lists of basic tokens for individual functions\n",
    "def getFunctions(contract):\n",
    "    lexer = pygments.lexers.get_lexer_by_name('Solidity')\n",
    "    tokens = list(pygments.lex(contract, lexer))\n",
    "    functionList = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if str(tokens[i][0]) == \"Token.Keyword.Type\" and tokens[i][1] == 'function':\n",
    "            currFunctionNum = len(functionList)\n",
    "            functionList.append([tokens[i]])\n",
    "            i += 1\n",
    "            \n",
    "            oneLiner = False\n",
    "            while tokens[i][1] != '{':\n",
    "                # the one-liner case\n",
    "                if str(tokens[i][1]) == ';':\n",
    "                    i += 1\n",
    "                    oneLiner = True\n",
    "                    break\n",
    "                if str(tokens[i][0]) != 'Token.Text.Whitespace' and str(tokens[i][0]) != 'Token.Comment.Single':\n",
    "                    functionList[currFunctionNum].append(tokens[i])\n",
    "                i += 1\n",
    "                \n",
    "            functionList[currFunctionNum].append(tokens[i])\n",
    "            openBracketsCount = 1\n",
    "            i += 1\n",
    "            \n",
    "            while openBracketsCount > 0:\n",
    "                if(tokens[i][1] == '{'): openBracketsCount += 1\n",
    "                elif(tokens[i][1] == '}'): openBracketsCount -= 1\n",
    "                \n",
    "                if str(tokens[i][0]) != 'Token.Text.Whitespace' and str(tokens[i][0]) != 'Token.Comment.Single':\n",
    "                    functionList[currFunctionNum].append(tokens[i])\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return functionList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a function's basic tokenization and a list of modifier and global variable names, fully tokenize function\n",
    "def tokenize(parsedFunction, globalVariablesList, modifierNameList):\n",
    "    txKeywords = ['call', 'value', 'transfer', 'send']\n",
    "    tokenList = []\n",
    "    for tup in parsedFunction:\n",
    "        if tokenList and tokenList[-1] == \"function\":\n",
    "            tokenList.append(\"Token.FunctionName\")\n",
    "        elif str(tup[1]) in txKeywords:\n",
    "            tokenList.append(tup[1])\n",
    "        elif 'Token.Literal' in str(tup[0]):\n",
    "            tokenList.append(\"Token.Constant\")\n",
    "        elif tup[1] in globalVariablesList:\n",
    "            tokenList.append(\"Token.GlobalVariable\")\n",
    "        elif tup[1] in modifierNameList:\n",
    "            tokenList.append(\"Token.ModifierName\")\n",
    "        elif tup[1].startswith(\"\\\"\"):\n",
    "            tokenList.append(\"Token.String\")\n",
    "        elif str(tup[0]) == 'Token.Name.Variable' or str(tup[0]) == 'Token.Text':\n",
    "            tokenList.append(\"Token.LocalVariable\")\n",
    "        elif str(tup[0]) != 'Token.Text.Whitespace' and str(tup[0]) != 'Token.Comment.Single':\n",
    "            tokenList.append(tup[1])\n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if keyword is for variable declaration (helper for global variable parser)\n",
    "def isType(word):\n",
    "    keywords = ['address', 'bool', 'byte', 'bytes', 'int', 'string', 'uint', 'mapping']\n",
    "    for kw in keywords:\n",
    "        if kw in word:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing pipeline:\n",
    "# - receives contract\n",
    "# - extracts list of global variable names\n",
    "# - extracts list of modifier names\n",
    "# - extracts list of function source code\n",
    "# - creates tokenized list of functions\n",
    "\n",
    "def tokenizeContractFunctions(contract):\n",
    "    globalVariablesList = getGlobalVariables(contract)\n",
    "    modifierNameList = getModifiers(contract)\n",
    "    functionList = getFunctions(contract)\n",
    "    \n",
    "    tokenizedFunctionList = []\n",
    "    for function in functionList:\n",
    "        tokenizedFunction = tokenize(function, globalVariablesList, modifierNameList)\n",
    "        tokenizedFunctionList.append(tokenizedFunction)\n",
    "    \n",
    "    return tokenizedFunctionList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame (currently not in use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTuples(tokenizedFunctions, labels):\n",
    "    tuples = []\n",
    "    for i in range(len(tokenizedFunctions)):\n",
    "        \n",
    "        tuples.append([tokenizedFunctions[i], labels[i]])\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame(contracts, contractLabels):\n",
    "    df = pd.DataFrame(columns=['tokenized_function', 'label'])\n",
    "    for i in range(len(contracts)):\n",
    "        tokenizedFunctions = tokenizeContractFunctions(contracts[i])\n",
    "        tuples = createTuples(tokenizedFunctions, contractLabels[i])\n",
    "        appendable = pd.DataFrame(tuples, columns=['tokenized_function', 'label'])\n",
    "        df = df.append(appendable, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Matching Lists of Functions and Corresponding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnData(contracts, contractLabels):\n",
    "    functions = []\n",
    "    labels = []\n",
    "    for i in range(len(contracts)):\n",
    "        tokenizedFunctions = tokenizeContractFunctions(contracts[i])\n",
    "        functions.extend(tokenizedFunctions)\n",
    "        labels.extend(contractLabels[i])\n",
    "    \n",
    "    return [functions, labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings for Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates Word2Vec mapping from token to vector\n",
    "def get_w2v_mapping(tokenizedFunctionsDf, token_dim):\n",
    "    w2v = Word2Vec(tokenizedFunctionsDf, min_count=1, size=token_dim, workers=3, window=3, sg=1)\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given list of tokenized functions, convert into list of list of embeddings\n",
    "def vectorize_functions(tokenized_functions, w2v_mapping):\n",
    "    embedding_list = []\n",
    "    for i in range(len(tokenized_functions)):\n",
    "        embedding = []\n",
    "        for token in tokenized_functions[i]:\n",
    "            embedding.append(w2v_mapping[token])\n",
    "        \n",
    "        embedding_list.append(embedding)\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given list of embeddings, add padding\n",
    "def pad_embeddings(embeddings, max_length, token_dim):\n",
    "    padded_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        zero_padding_cnt = max_length - len(embedding)\n",
    "        pad = np.zeros((1, token_dim))\n",
    "        for i in range(zero_padding_cnt):\n",
    "            embedding = np.concatenate((embedding, pad), axis=0)\n",
    "        padded_embeddings.append(embedding)\n",
    "    return padded_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiles Word2Vec mapping, creates embedded function representations, and applies padding\n",
    "def getFunctionEmbeddings(tokenizedFunctions, max_length, token_dim):\n",
    "    w2v_mapping = get_w2v_mapping(tokenizedFunctions, token_dim)\n",
    "    vectorized_fns = vectorize_functions(tokenizedFunctions, w2v_mapping)\n",
    "    padded_embeddings = pad_embeddings(vectorized_fns, max_length, token_dim)\n",
    "    return [w2v_mapping, padded_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (432, 500, 15)\n"
     ]
    }
   ],
   "source": [
    "length = 500\n",
    "token_dim = 15\n",
    "\n",
    "[functions, labels] = returnData(reentrancyContracts.contracts, reentrancyContractLabels.labels)\n",
    "[w2v_mapping, fn_embeddings] = getFunctionEmbeddings(functions, length, token_dim)\n",
    "fn_embeddings = np.array(fn_embeddings)\n",
    "print(\"X shape: \", fn_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, regularizers, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional, LSTM, Flatten, Embedding, Dropout\n",
    "from keras.metrics import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile formatted data and create splits\n",
    "[functions, labels] = returnData(reentrancyContracts.contracts, reentrancyContractLabels.labels)\n",
    "[w2v_mapping, X_train] = getFunctionEmbeddings(functions, 500, 15)\n",
    "\n",
    "X = np.array(X_train)\n",
    "y = to_categorical(np.array(labels))\n",
    "X_train = X[30:]\n",
    "y_train = y[30:]\n",
    "X_test = X[:30]\n",
    "y_test = y[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(402, 500, 15)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(402, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Bidirectional(LSTM(300)))\n",
    "model_1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model_1.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "history = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on train data\n",
      "402/402 [==============================] - 3s 7ms/step\n",
      "train loss, train acc: [0.47703068603330584, 0.7786069512367249]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on train data\")\n",
    "results = model_1.evaluate(X_train, y_train)\n",
    "print(\"train loss, train acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "30/30 [==============================] - 0s 7ms/step\n",
      "test loss, test acc: [0.8837546110153198, 0.46666666865348816]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model_1.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(w2v_mapping.wv.vocab)\n",
    "embedding_dim = 15\n",
    "embeddings_per_example = 500\n",
    "\n",
    "metrics = [\n",
    "    Precision(name='precision'),\n",
    "    Recall(name='recall')\n",
    "]\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model_2.add(layers.GlobalMaxPooling1D())\n",
    "model_2.add(layers.Dense(10, activation='relu'))\n",
    "model_2.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Training...\")\n",
    "history = model_2.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=30, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on train data\n",
      "402/402 [==============================] - 0s 106us/step\n",
      "train loss, train acc: [0.16532784878781334, 0.9402984976768494]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on train data\")\n",
    "results = model_2.evaluate(X_train, y_train)\n",
    "print(\"train loss, train acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "30/30 [==============================] - 0s 95us/step\n",
      "test loss, test acc: [1.1498416662216187, 0.800000011920929]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model_2.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
